{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Категориальные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова будем использовать немного видоизменённый для наших целей датасет https://archive.ics.uci.edu/ml/datasets/AutoUniv.\n",
    "\n",
    "В нём присутствуют целочисленные, вещественнозначные и категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('table.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_features_mask = (df.dtypes == \"object\").values\n",
    "cat_features_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cat_features_mask[cat_features_mask==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перевод текстовых категориальных признаков в числа**\n",
    "\n",
    "Переведем все нечисловые признаки в числовые (порядковые), а затем будем их кодировать различными способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_enc = preprocessing.LabelEncoder()\n",
    "for feature in df.columns[cat_features_mask]: \n",
    "    df[feature] = label_enc.fit_transform(df[feature])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHot-кодирование**\n",
    "\n",
    "Пусть некоторый признак принимает значения из множества K. OneHotEncoder вместо одного признака создает K бинарных признаков - по одному на каждое возможное значение исходного признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "df_cat = enc.fit_transform(df[df.columns[cat_features_mask]])\n",
    "df_cat = pd.DataFrame(data=df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_cat.shape\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Хэширование**\n",
    "\n",
    "HashingVectorizer преобразовывает строку в числовой массив заданной длиной с помощью хэш-функции. В этом методе в качестве входных параметров мы задаем желаемое количество новых признаков, а также токенизатор - обработчик текста (в нём мы можем сделать любую удобную нам предобработку текста: удалить редкие слова, удалить знаки препинания, оставить только слова из определенного списка и т.д.). Токенизатор возвращает текст, разбитый на токены, т.е. на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return [elem for elem in s.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее интересный для нас с точки зрения хэширования - столбец att9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('table.csv')\n",
    "\n",
    "#Выведите на экран все различные значения элементов из столбца 'att9'\n",
    "#Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для применения HashingVectorize выбираем из столбца все различные значения(слова) без повторений, обучаем HashingVectorizer на этих словах и применяем ко всему столбцу. В итоге мы получаем разреженную матрицу. С ней умеют работать многие алгоритмы машинного обучения, но при желании можем перевести ее в numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "coder = HashingVectorizer(tokenizer=my_tokenizer, n_features=15)\n",
    "\n",
    "TrainNotDuble = df['att9'].drop_duplicates()\n",
    "coder.fit(TrainNotDuble)\n",
    "\n",
    "coder.transform(df['att9'].values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из направлений машинного обучения является работа с текстами и извлечение полезной информации из текстов. Чтобы алгоритмы машинного обучения могли работать с текстами, необходимо перевести тексты в наборы чисел. Для этого применяют различные алгоритмы векторизации текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем изучать датасет, содержащий отзывы о фильмах. Отзывы могут быть положительные, либо отрицательные. Наша конечная задача - научиться различать положительные и отрицательные отзывы.\n",
    "\n",
    "Загрузим датасет и уберем из него плохие строки (в которых нет оценки фильму)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "X = []\n",
    "y = []\n",
    "import codecs \n",
    "\n",
    "bad = 0\n",
    "with codecs.open('imdb_labelled.txt',encoding='utf-8') as thefile:\n",
    "    for row in tqdm(thefile.readlines()):\n",
    "        try:\n",
    "            resp_curr, y_curr = row.split('\\t')\n",
    "        except:\n",
    "            'ValueError'\n",
    "            print row\n",
    "            bad+=1\n",
    "        X.append(resp_curr)\n",
    "        y.append(int(y_curr))\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_responses = filter(lambda r: 'awful' in r, X)\n",
    "print bad_responses[1]\n",
    "\n",
    "#Выведите на экран несколько строк со словами 'bad' и 'great'. \n",
    "#Верно ли, что все отзывы со словом 'bad' негативные, а отзывы со словом 'great' позитивные?\n",
    "#Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые этапы обработки текста:\n",
    "\n",
    "* снижение регистра\n",
    "\n",
    "* удаление пунктуации\n",
    "\n",
    "* удаление всех символов, кроме символов нашего алфавита (в данном случае, латинского)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print re.sub(ur'[^\\p{Latin}]', ' ', bad_responses[1].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Texts = map(lambda r: re.sub(ur'[^\\p{Latin}]', ' ', r.lower()), X)\n",
    "Texts[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение ответов в наших данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 способ векторизации: счётчик (CountVectorizer)**\n",
    "\n",
    "Каждому слову соответствует количество его вхождений в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(Texts)\n",
    "vectorizer.transform(Texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vectorizer.transform(Texts[:1]).indptr\n",
    "print vectorizer.transform(Texts[:1]).indices\n",
    "print vectorizer.transform(Texts[:1]).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 способ векторизации: TF-IDF**\n",
    "\n",
    "Ещё один способ работы с текстовыми данными — TF-IDF (Term Frequency–Inverse Document Frequency). Рассмотрим коллекцию текстов $D$. Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n",
    "\n",
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте: \n",
    "    $$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$ где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "2. Inverse Document Frequency $$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$ где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n",
    "\n",
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину: $$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D).$\n",
    "\n",
    "Признаковым описанием одного объекта $d \\in D$ будет вектор $\\bigg(\\text{tf-idf}(t,d, D)\\bigg)_{t\\in V}$, где $V$ – словарь всех слов, встречающихся в коллекции $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Объявите TrIdfVectorizer с теми же параметрами, что и CountVectorizer\n",
    "#Your code is here\n",
    "vectorizer = TfidfVectorizer(...)\n",
    "\n",
    "#Сделайте fit и predict, как было сделано в предыдущем примере\n",
    "#Your code is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print vectorizer.transform(Texts[:1]).indptr\n",
    "print vectorizer.transform(Texts[:1]).indices\n",
    "print vectorizer.transform(Texts[:1]).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим два рассмотренных метода векторизации к задаче классификации отзывов на два класса (положительные и отрицательные)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(Texts)\n",
    "\n",
    "X = vectorizer.transform(Texts)\n",
    "y = np.array(y)\n",
    "\n",
    "cv = ShuffleSplit(X.shape[0], n_iter=1, test_size=0.2)\n",
    "\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], y[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print 'ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y[test_ids], preds), \n",
    "                                        accuracy_score(y[test_ids], (preds > 0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(Texts)\n",
    "\n",
    "X = vectorizer.transform(Texts)\n",
    "y = np.array(y)\n",
    "\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], y[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print 'ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y[test_ids], preds), \n",
    "                                        accuracy_score(y[test_ids], (preds > 0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важность признаков**\n",
    "\n",
    "В задачах, связанных с обработкой текстов, признаки как правило хорошо интерпретируемы. Для визуального контроля качества работы алгоритма можно посмотреть на те слова, которые алгоритм посчитал наиболее важными для данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "weights = sorted(weights, key=lambda i: i[1])\n",
    "for i in range(1,20):\n",
    "    print '%s, %.2f' % weights[-i]\n",
    "    \n",
    "print '...'\n",
    "for i in reversed(range(1,20)):\n",
    "    print '%s, %.2f' % weights[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 способ векторизации: Word2Vec**\n",
    "\n",
    "Word2Vec - это алгоритм, который собирает статистику по совместному появлению слов в фразах, а затем с помощью нейронных сетей решает задачу снижения размерности и выдает на выходе компактные векторные представления слов, в максимальной степени отражающие отношения этих слов в обрабатываемых текстах.\n",
    "\n",
    "Нахождение связей между контекстами слов основано на предположении, что слова, находящиеся в похожих контекстах, имеют тенденцию значить похожие вещи, т.е. быть семантически близкими. \n",
    "Формально задача стоит так: максимизировать косинусное расстояние между векторами слов (скалярное произведение векторов), которые появляются рядом друг с другом, и минимизировать косинусное расстояние между векторами слов, которые не появляются друг рядом с другом. Рядом друг с другом в данном случае значит в близких контекстах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим векторизацию с помощью word2vec для наших данных. Кроме того, удалим stop-слова, то есть слова, часто встречающиеся во всех английских текстах - это ещё один полезный метод обработки текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_stopwords(review, remove_stopwords=True):\n",
    "    \n",
    "    words = review.split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем строки нашего датасета в токенизированный вид и удалим из них стоп-слова - в этом виде они пригодны для использования word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=True ):\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(delete_stopwords(sentence,remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "Y = []\n",
    "for i in range(len(Texts)):\n",
    "    if len(set(Texts[i])) == 1:\n",
    "        continue\n",
    "    Y.append(y[i])\n",
    "    sentences += review_to_sentences(Texts[i], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим word2vec к токенизированному корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = 500                       \n",
    "min_word_count = 5                   \n",
    "num_workers = 4       \n",
    "context = 10                                                                               \n",
    "downsampling = 1e-5   \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = str(num_features)+\"features_word2vec\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь каждое слово корпуса имеет векторное представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве одного из способов векторизовать текст (в нашем случае отзыв на фильм), можно усреднить векторы слов, входящих в этот текст. Так и сделаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "index2word_set = set(model.wv.index2word)\n",
    "\n",
    "def make_featurevec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec += model[word]\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def get_avg_featurevecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = make_featurevec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "trainDataVecs = get_avg_featurevecs(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, обучим классификатор на полученных признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(trainDataVecs.shape[0], n_iter=1, test_size=0.2)\n",
    "\n",
    "Y = np.array(Y)\n",
    "\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(trainDataVecs[train_ids], Y[train_ids])\n",
    "    preds = lr.predict_proba(trainDataVecs[test_ids])[:,1]\n",
    "    print 'ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(Y[test_ids], preds), \n",
    "                                        accuracy_score(Y[test_ids], (preds > 0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Попробуйте обучить word2vec не на 500 признаках, а на 100, 250, 1000. Какое количество признаков дает наилучшее качество модели?\n",
    "#Напишите здесь вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация кластеров слов с признаками из Word2Vec, с помощью t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = fetch_20newsgroups()\n",
    "\n",
    "def clean(text):\n",
    "    \"\"\"Remove posting header, split by sentences and words, keep only letters\"\"\"\n",
    "    lines = re.split('[?!.:]\\s', re.sub('^.*Lines: \\d+', '', re.sub('\\n', ' ', text)))\n",
    "    return [re.sub('[^a-zA-Z]', ' ', line).lower().split() for line in lines]\n",
    "\n",
    "sentences = [line for text in train.data for line in clean(text)]\n",
    "\n",
    "model = Word2Vec(sentences, workers=4, size=100, min_count=50, window=10, sample=1e-3)\n",
    "\n",
    "print (model.most_similar('day'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "vocab = [elem for elem in model.wv.index2word if elem not in stop]\n",
    "vocab = [elem for elem in vocab if len(elem) >= 5]\n",
    "print len(vocab)\n",
    "\n",
    "X = model[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proj = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "Y = proj.fit_transform(X[:500]) \n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.scatter(Y[:, 0], Y[:, 1],color='white')\n",
    "for label, x, y in zip(vocab, Y[:, 0], Y[:, 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
